{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2c0a8f",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8f9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff552ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATHS = [\n",
    "  \"data/Emotion NLP/goemotions_1.csv\",\n",
    "  \"data/Emotion NLP/goemotions_2.csv\",\n",
    "  \"data/Emotion NLP/goemotions_3.csv\",\n",
    "]\n",
    "\n",
    "KEPT_LABELS = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"love\"]\n",
    "\n",
    "TARGET_MIN_SAMPLES = 1000\n",
    "TARGET_MAX_SAMPLES = 3000\n",
    "MAX_LABEL_FRACTION = 0.30  # no emotion should dominate >30%\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f484a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 211225 total samples\n"
     ]
    }
   ],
   "source": [
    "dfs = [pd.read_csv(path) for path in CSV_PATHS]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} total samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ab6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 3411 unclear samples\n"
     ]
    }
   ],
   "source": [
    "if \"example_very_unclear\" in df.columns:\n",
    "  before = len(df)\n",
    "  df = df[df[\"example_very_unclear\"] == False]\n",
    "  print(f\"Dropped {before - len(df)} unclear samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c97ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\"id\", \"text\"] + KEPT_LABELS\n",
    "df = df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7dbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 169556 zero-label samples\n"
     ]
    }
   ],
   "source": [
    "label_sum = df[KEPT_LABELS].sum(axis=1)\n",
    "before = len(df)\n",
    "df = df[label_sum > 0]\n",
    "print(f\"Dropped {before - len(df)} zero-label samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9b326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56dd546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_frequencies(frame):\n",
    "  return frame[KEPT_LABELS].sum() / len(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361aab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final label distribution:\n",
      "love        8191\n",
      "anger       8084\n",
      "joy         7983\n",
      "sadness     6758\n",
      "surprise    5514\n",
      "fear        3197\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  freqs = label_frequencies(df)\n",
    "  max_label = freqs.idxmax()\n",
    "  max_frac = freqs[max_label]\n",
    "\n",
    "  if max_frac <= MAX_LABEL_FRACTION:\n",
    "    break\n",
    "\n",
    "  # find rows dominated only by the max label\n",
    "  mask = (df[max_label] == 1) & (df[KEPT_LABELS].sum(axis=1) == 1)\n",
    "  removable = df[mask]\n",
    "\n",
    "  if len(removable) == 0:\n",
    "    break\n",
    "\n",
    "  # remove a small chunk\n",
    "  drop_n = min(50, len(removable))\n",
    "  drop_idx = removable.sample(n=drop_n, random_state=RANDOM_SEED).index\n",
    "  df = df.drop(drop_idx)\n",
    "  \n",
    "\n",
    "print(\"Final label distribution:\")\n",
    "print(df[KEPT_LABELS].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abbe773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 3000\n"
     ]
    }
   ],
   "source": [
    "if len(df) > TARGET_MAX_SAMPLES:\n",
    "  df = df.sample(n=TARGET_MAX_SAMPLES, random_state=RANDOM_SEED)\n",
    "\n",
    "elif len(df) < TARGET_MIN_SAMPLES:\n",
    "  print(\"Warning: dataset smaller than target minimum\")\n",
    "\n",
    "print(f\"Final dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e55c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frozen evaluation set → data/Emotion NLP/goemotions_eval_set.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"data/Emotion NLP/goemotions_eval_set.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved frozen evaluation set → {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5568b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5c4fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d2a13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_LABELS = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "ROBERTA_LABELS = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "MODEL_NAMES = {\n",
    "  \"distilbert\": [\"bhadresh-savani/distilbert-base-uncased-emotion\", DIST_LABELS],\n",
    "  \"roberta\": [\"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\", ROBERTA_LABELS],\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "THRESHOLD = 0.5\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "DATA_PATH = \"data/Emotion NLP/goemotions_eval_set.csv\"\n",
    "OUTPUT_DIR = \"data/Emotion NLP/results/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aecbd2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 samples\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "ids = df[\"id\"].tolist()\n",
    "\n",
    "n_samples = len(df)\n",
    "print(f\"Loaded {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1de8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, texts, tokenizer):\n",
    "    self.texts = texts\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.tokenizer(\n",
    "      self.texts[idx],\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      max_length=MAX_LENGTH,\n",
    "      return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b286705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_name, hf_name, labels_len):\n",
    "  print(f\"\\nRunning inference: {model_name}\")\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"Device: {device}\")\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(hf_name)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    hf_name,\n",
    "    num_labels=labels_len,\n",
    "    problem_type=\"multi_label_classification\"\n",
    "  )\n",
    "\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  dataset = TextDataset(texts, tokenizer)\n",
    "  dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "  all_probs = []\n",
    "\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "      batch = {k: v.squeeze(1).to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      logits = outputs.logits\n",
    "      probs = torch.sigmoid(logits)\n",
    "      all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "  end_time = time.perf_counter()\n",
    "\n",
    "  probs = np.vstack(all_probs)\n",
    "\n",
    "  timing = {\n",
    "    \"model\": hf_name,\n",
    "    \"device\": str(device),\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_samples\": n_samples,\n",
    "    \"total_inference_time_sec\": round(end_time - start_time, 3),\n",
    "    \"avg_time_per_sample_ms\": round(\n",
    "      (end_time - start_time) / n_samples * 1000, 3\n",
    "    )\n",
    "  }\n",
    "\n",
    "  return probs, timing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfcedf",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a2f6718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: distilbert\n",
      "Device: cpu\n",
      "Probabilities shape: (5, 6)\n",
      "Sample probabilities:\n",
      " [[0.15915503 0.99852896 0.28175303 0.1580044  0.0782557  0.12352173]\n",
      " [0.20308957 0.99863285 0.25943714 0.13026811 0.08543865 0.11956318]]\n",
      "Sample predictions:\n",
      " [[0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]]\n",
      "\n",
      "TEXT: Rusty you the bomb! I'm okish now but just knowing you are out there makes me smile! Cheers! xoxo\n",
      "sadness    → 0.159\n",
      "joy        → 0.999\n",
      "love       → 0.282\n",
      "anger      → 0.158\n",
      "fear       → 0.078\n",
      "surprise   → 0.124\n",
      "\n",
      "TEXT: happy birthday, have some gold!\n",
      "sadness    → 0.203\n",
      "joy        → 0.999\n",
      "love       → 0.259\n",
      "anger      → 0.130\n",
      "fear       → 0.085\n",
      "surprise   → 0.120\n",
      "\n",
      "TEXT: You're a mean person.\n",
      "sadness    → 0.325\n",
      "joy        → 0.171\n",
      "love       → 0.140\n",
      "anger      → 0.998\n",
      "fear       → 0.227\n",
      "surprise   → 0.074\n",
      "\n",
      "TEXT: Honestly, State and Revolution is what made me ananarchist.\n",
      "sadness    → 0.314\n",
      "joy        → 0.910\n",
      "love       → 0.104\n",
      "anger      → 0.946\n",
      "fear       → 0.197\n",
      "surprise   → 0.057\n",
      "\n",
      "TEXT: that's ace, thank you so much \n",
      "sadness    → 0.166\n",
      "joy        → 0.996\n",
      "love       → 0.700\n",
      "anger      → 0.152\n",
      "fear       → 0.051\n",
      "surprise   → 0.074\n"
     ]
    }
   ],
   "source": [
    "SANITY_N = 5\n",
    "df_sanity = df.head(SANITY_N)\n",
    "\n",
    "texts = df_sanity[\"text\"].tolist()\n",
    "ids = df_sanity[\"id\"].tolist()\n",
    "\n",
    "\n",
    "probs, timing = run_inference(\n",
    "  model_name=\"distilbert\",\n",
    "  hf_name=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "  labels_len=len(DIST_LABELS)\n",
    ")\n",
    "\n",
    "print(\"Probabilities shape:\", probs.shape)\n",
    "print(\"Sample probabilities:\\n\", probs[:2])\n",
    "\n",
    "preds = (probs >= THRESHOLD).astype(int)\n",
    "print(\"Sample predictions:\\n\", preds[:2])\n",
    "\n",
    "for i in range(df_sanity.shape[0]):\n",
    "  print(\"\\nTEXT:\", texts[i])\n",
    "  for j, label in enumerate(DIST_LABELS):\n",
    "    print(f\"{label:10s} → {probs[i][j]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59feea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: roberta\n",
      "Device: cpu\n",
      "Probabilities shape: (5, 11)\n",
      "Sample probabilities:\n",
      " [[0.0184311  0.03651299 0.0166482  0.01362048 0.98732346 0.8460089\n",
      "  0.7814287  0.01076462 0.01864295 0.04037486 0.12000268]\n",
      " [0.01707987 0.03240285 0.01807778 0.01351091 0.98402    0.8782865\n",
      "  0.6787714  0.01236995 0.02928596 0.03801468 0.09515911]]\n",
      "Sample predictions:\n",
      " [[0 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 0 0 0 0]]\n",
      "\n",
      "TEXT: Rusty you the bomb! I'm okish now but just knowing you are out there makes me smile! Cheers! xoxo\n",
      "sadness    → 0.018\n",
      "joy        → 0.037\n",
      "love       → 0.017\n",
      "anger      → 0.014\n",
      "fear       → 0.987\n",
      "surprise   → 0.846\n",
      "\n",
      "TEXT: happy birthday, have some gold!\n",
      "sadness    → 0.017\n",
      "joy        → 0.032\n",
      "love       → 0.018\n",
      "anger      → 0.014\n",
      "fear       → 0.984\n",
      "surprise   → 0.878\n",
      "\n",
      "TEXT: You're a mean person.\n",
      "sadness    → 0.980\n",
      "joy        → 0.023\n",
      "love       → 0.931\n",
      "anger      → 0.039\n",
      "fear       → 0.018\n",
      "surprise   → 0.006\n",
      "\n",
      "TEXT: Honestly, State and Revolution is what made me ananarchist.\n",
      "sadness    → 0.052\n",
      "joy        → 0.592\n",
      "love       → 0.049\n",
      "anger      → 0.021\n",
      "fear       → 0.279\n",
      "surprise   → 0.014\n",
      "\n",
      "TEXT: that's ace, thank you so much \n",
      "sadness    → 0.018\n",
      "joy        → 0.072\n",
      "love       → 0.014\n",
      "anger      → 0.009\n",
      "fear       → 0.989\n",
      "surprise   → 0.539\n"
     ]
    }
   ],
   "source": [
    "probs, timing = run_inference(\n",
    "  model_name=\"roberta\",\n",
    "  hf_name=\"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\",\n",
    "  labels_len=len(ROBERTA_LABELS)\n",
    ")\n",
    "\n",
    "print(\"Probabilities shape:\", probs.shape)\n",
    "print(\"Sample probabilities:\\n\", probs[:2])\n",
    "\n",
    "preds = (probs >= THRESHOLD).astype(int)\n",
    "print(\"Sample predictions:\\n\", preds[:2])\n",
    "\n",
    "for i in range(df_sanity.shape[0]):\n",
    "  print(\"\\nTEXT:\", texts[i])\n",
    "  for j, label in enumerate(DIST_LABELS):\n",
    "    print(f\"{label:10s} → {probs[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685bba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# # Load tokenizer\n",
    "# distil_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Load model\n",
    "# distil_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "#   \"distilbert-base-uncased\",\n",
    "#   problem_type=\"multi_label_classification\",  # for multi-label\n",
    "#   num_labels=len(kept_labels)               # your 8 labels\n",
    "# )\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# Load tokenizer\n",
    "distil_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load model\n",
    "distil_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "  \"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "  problem_type=\"multi_label_classification\",  # for multi-label\n",
    "  num_labels=len([\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])               # your 8 labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be241bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "\n",
    "# # Load tokenizer\n",
    "# roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# # Load model\n",
    "# roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
    "#   \"roberta-base\",\n",
    "#   problem_type=\"multi_label_classification\",\n",
    "#   num_labels=len(kept_labels)\n",
    "# )\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "\n",
    "# Load tokenizer\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load model\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
    "  \"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\",\n",
    "  problem_type=\"multi_label_classification\",\n",
    "  num_labels=len([\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d02f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick inference\n",
    "import torch\n",
    "\n",
    "def predict(model, tokenizer, texts, threshold=0.5):\n",
    "  inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits)  # multi-label probabilities\n",
    "    preds = (probs >= threshold).int()\n",
    "  return preds, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12692b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0]], dtype=torch.int32),\n",
       " tensor([[0.2172, 0.9986, 0.2412, 0.1349, 0.0895, 0.1017],\n",
       "         [0.9974, 0.1571, 0.0907, 0.4971, 0.2588, 0.0890]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])\n",
    "predict(distil_model, distil_tokenizer, [\"I am so happy today!\", \"This is terrible...\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9de69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]], dtype=torch.int32),\n",
       " tensor([[0.0148, 0.0433, 0.0136, 0.0122, 0.9900, 0.6960, 0.8425, 0.0085, 0.0150,\n",
       "          0.0384, 0.1132],\n",
       "         [0.9562, 0.0266, 0.9608, 0.6311, 0.0089, 0.0081, 0.0064, 0.1003, 0.5182,\n",
       "          0.0625, 0.0075]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"])\n",
    "predict(roberta_model, roberta_tokenizer, [\"I am so happy today!\", \"This is terrible...\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d54b52",
   "metadata": {},
   "source": [
    "## Full Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef131dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: distilbert\n",
      "Device: cpu\n",
      "Saved → data/Emotion NLP/results//distilbert_predictions.csv\n",
      "Saved → data/Emotion NLP/results//distilbert_timing.json\n",
      "\n",
      "Running inference: roberta\n",
      "Device: cpu\n",
      "Saved → data/Emotion NLP/results//roberta_predictions.csv\n",
      "Saved → data/Emotion NLP/results//roberta_timing.json\n"
     ]
    }
   ],
   "source": [
    "for short_name, (hf_name, labels) in MODEL_NAMES.items():\n",
    "\n",
    "  probs, timing = run_inference(short_name, hf_name, len(labels))\n",
    "\n",
    "  # Convert probabilities → predictions\n",
    "  preds = (probs >= THRESHOLD).astype(int)\n",
    "\n",
    "  # Build output dataframe\n",
    "  out_df = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"text\": texts\n",
    "  })\n",
    "\n",
    "  for i, label in enumerate(labels):\n",
    "    out_df[f\"conf_{label}\"] = probs[:, i]\n",
    "    out_df[f\"pred_{label}\"] = preds[:, i]\n",
    "\n",
    "  # Save predictions\n",
    "  pred_path = f\"{OUTPUT_DIR}/{short_name}_predictions.csv\"\n",
    "  out_df.to_csv(pred_path, index=False)\n",
    "\n",
    "  # Save timing\n",
    "  timing_path = f\"{OUTPUT_DIR}/{short_name}_timing.json\"\n",
    "  with open(timing_path, \"w\") as f:\n",
    "    json.dump(timing, f, indent=2)\n",
    "\n",
    "  print(f\"Saved → {pred_path}\")\n",
    "  print(f\"Saved → {timing_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482deae1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c167cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (precision_recall_fscore_support,\n",
    "                             classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e25478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel(\n",
    "  df_preds,\n",
    "  df_gold,\n",
    "  labels,\n",
    "  threshold=0.5):\n",
    "  \"\"\"\n",
    "  df_preds: dataframe with conf_<label> columns\n",
    "  df_gold: dataframe with true <label> columns\n",
    "  labels: list of labels to evaluate\n",
    "  \"\"\"\n",
    "\n",
    "  # Build prediction matrix\n",
    "  y_pred = np.vstack([\n",
    "    (df_preds[f\"conf_{lbl}\"] >= threshold).astype(int)\n",
    "    for lbl in labels\n",
    "  ]).T\n",
    "\n",
    "  # Build gold matrix\n",
    "  y_true = df_gold[labels].values\n",
    "\n",
    "  # Macro + micro\n",
    "  macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"macro\", zero_division=0\n",
    "  )\n",
    "\n",
    "  micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"micro\", zero_division=0\n",
    "  )\n",
    "\n",
    "  overall = pd.DataFrame([{\n",
    "    \"macro_precision\": macro_p,\n",
    "    \"macro_recall\": macro_r,\n",
    "    \"macro_f1\": macro_f1,\n",
    "    \"micro_precision\": micro_p,\n",
    "    \"micro_recall\": micro_r,\n",
    "    \"micro_f1\": micro_f1\n",
    "  }])\n",
    "\n",
    "  # Per-label metrics\n",
    "  per_label = []\n",
    "  for i, lbl in enumerate(labels):\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "      y_true[:, i],\n",
    "      y_pred[:, i],\n",
    "      average=\"binary\",\n",
    "      zero_division=0\n",
    "    )\n",
    "    per_label.append({\n",
    "      \"label\": lbl,\n",
    "      \"precision\": p,\n",
    "      \"recall\": r,\n",
    "      \"f1\": f1\n",
    "    })\n",
    "\n",
    "  per_label_df = pd.DataFrame(per_label)\n",
    "\n",
    "  return overall, per_label_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT - Overall\n",
      "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
      "0         0.413491      0.605861  0.435764         0.357182       0.62784   \n",
      "\n",
      "   micro_f1  \n",
      "0  0.455326  \n",
      "\n",
      "DistilBERT - Per emotion F1\n",
      "      label  precision    recall        f1\n",
      "4  surprise   0.512438  0.227876  0.315467\n",
      "3      fear   0.251603  0.554770  0.346196\n",
      "0       joy   0.310972  0.879421  0.459471\n",
      "2     anger   0.327097  0.889984  0.478376\n",
      "5      love   0.666667  0.401899  0.501481\n",
      "1   sadness   0.412170  0.681214  0.513591\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413491</td>\n",
       "      <td>0.605861</td>\n",
       "      <td>0.435764</td>\n",
       "      <td>0.357182</td>\n",
       "      <td>0.62784</td>\n",
       "      <td>0.455326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
       "0         0.413491      0.605861  0.435764         0.357182       0.62784   \n",
       "\n",
       "   micro_f1  \n",
       "0  0.455326  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distil_preds = pd.read_csv(\"data/Emotion NLP/results/distilbert_predictions.csv\")\n",
    "df_gold = pd.read_csv(\"data/Emotion NLP/goemotions_eval_set.csv\")\n",
    "\n",
    "distil_overall, distil_per_label = evaluate_multilabel(\n",
    "  df_preds=df_distil_preds,\n",
    "  df_gold=df_gold,\n",
    "  labels=KEPT_LABELS,\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"DistilBERT - Overall\")\n",
    "print(distil_overall)\n",
    "\n",
    "print(\"\\nDistilBERT - Per emotion F1\")\n",
    "print(distil_per_label.sort_values(\"f1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "651fe1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_roberta_predictions(\n",
    "  df_roberta,\n",
    "  mapping,\n",
    "  threshold=0.5):\n",
    "  \"\"\"\n",
    "  Converts RoBERTa predictions into 6-label emotion space\n",
    "  \"\"\"\n",
    "\n",
    "  df_out = df_roberta[[\"id\", \"text\"]].copy()\n",
    "\n",
    "  for target_label, source_labels in mapping.items():\n",
    "    conf_cols = [f\"conf_{lbl}\" for lbl in source_labels]\n",
    "\n",
    "    # Max confidence across mapped labels\n",
    "    df_out[f\"conf_{target_label}\"] = df_roberta[conf_cols].max(axis=1)\n",
    "\n",
    "    # Binary prediction\n",
    "    df_out[f\"pred_{target_label}\"] = (\n",
    "      df_out[f\"conf_{target_label}\"] >= threshold\n",
    "    ).astype(int)\n",
    "\n",
    "  return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ddcefbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa - Overall\n",
      "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
      "0         0.518823       0.70346  0.577528         0.492351        0.7312   \n",
      "\n",
      "   micro_f1  \n",
      "0  0.588463  \n",
      "\n",
      "RoBERTa - Per emotion F1\n",
      "      label  precision    recall        f1\n",
      "3      fear   0.368664  0.565371  0.446304\n",
      "4  surprise   0.619632  0.446903  0.519280\n",
      "0       joy   0.388453  0.876206  0.538272\n",
      "2     anger   0.465442  0.873563  0.607306\n",
      "1   sadness   0.558651  0.722960  0.630273\n",
      "5      love   0.712098  0.735759  0.723735\n"
     ]
    }
   ],
   "source": [
    "ROBERTA_TO_6 = {\n",
    "  \"anger\": [\"anger\", \"disgust\"],\n",
    "  \"fear\": [\"fear\", \"pessimism\"],\n",
    "  \"joy\": [\"joy\", \"optimism\"],\n",
    "  \"love\": [\"love\", \"trust\"],\n",
    "  \"sadness\": [\"sadness\"],\n",
    "  \"surprise\": [\"surprise\", \"anticipation\"]\n",
    "}\n",
    "\n",
    "\n",
    "df_roberta_preds = pd.read_csv(\"data/Emotion NLP/results/roberta_predictions.csv\")\n",
    "\n",
    "df_roberta_6 = collapse_roberta_predictions(\n",
    "  df_roberta_preds,\n",
    "  ROBERTA_TO_6,\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "df_roberta_6.head()\n",
    "\n",
    "roberta_overall, roberta_per_label = evaluate_multilabel(\n",
    "  df_preds=df_roberta_6,\n",
    "  df_gold=df_gold,\n",
    "  labels=KEPT_LABELS,\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"RoBERTa - Overall\")\n",
    "print(roberta_overall)\n",
    "\n",
    "print(\"\\nRoBERTa - Per emotion F1\")\n",
    "print(roberta_per_label.sort_values(\"f1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbe30a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413491</td>\n",
       "      <td>0.605861</td>\n",
       "      <td>0.435764</td>\n",
       "      <td>0.357182</td>\n",
       "      <td>0.62784</td>\n",
       "      <td>0.455326</td>\n",
       "      <td>DistilBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.590085</td>\n",
       "      <td>0.659299</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.522958</td>\n",
       "      <td>0.69248</td>\n",
       "      <td>0.595897</td>\n",
       "      <td>RoBERTa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
       "0         0.413491      0.605861  0.435764         0.357182       0.62784   \n",
       "0         0.590085      0.659299  0.574200         0.522958       0.69248   \n",
       "\n",
       "   micro_f1       model  \n",
       "0  0.455326  DistilBERT  \n",
       "0  0.595897     RoBERTa  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_common_overall, distil_common_per_label = evaluate_multilabel(\n",
    "  df_distil_preds, df_gold, KEPT_LABELS\n",
    ")\n",
    "\n",
    "roberta_common_overall, roberta_common_per_label = evaluate_multilabel(\n",
    "  df_roberta_preds, df_gold, KEPT_LABELS\n",
    ")\n",
    "\n",
    "comparison = pd.concat([\n",
    "  distil_common_overall.assign(model=\"DistilBERT\"),\n",
    "  roberta_common_overall.assign(model=\"RoBERTa\")\n",
    "])\n",
    "\n",
    "comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
