{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8f9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c0a8f",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff552ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATHS = [\n",
    "  \"data/Emotion NLP/goemotions_1.csv\",\n",
    "  \"data/Emotion NLP/goemotions_2.csv\",\n",
    "  \"data/Emotion NLP/goemotions_3.csv\",\n",
    "]\n",
    "\n",
    "KEPT_LABELS = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"love\"]\n",
    "\n",
    "TARGET_MIN_SAMPLES = 1000\n",
    "TARGET_MAX_SAMPLES = 3000\n",
    "MAX_LABEL_FRACTION = 0.30  # no emotion should dominate >30%\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f484a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 211225 total samples\n"
     ]
    }
   ],
   "source": [
    "dfs = [pd.read_csv(path) for path in CSV_PATHS]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} total samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ab6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 3411 unclear samples\n"
     ]
    }
   ],
   "source": [
    "if \"example_very_unclear\" in df.columns:\n",
    "  before = len(df)\n",
    "  df = df[df[\"example_very_unclear\"] == False]\n",
    "  print(f\"Dropped {before - len(df)} unclear samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\"text\"] + KEPT_LABELS\n",
    "df = df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7dbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 169556 zero-label samples\n"
     ]
    }
   ],
   "source": [
    "label_sum = df[KEPT_LABELS].sum(axis=1)\n",
    "before = len(df)\n",
    "df = df[label_sum > 0]\n",
    "print(f\"Dropped {before - len(df)} zero-label samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9b326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56dd546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_frequencies(frame):\n",
    "  return frame[KEPT_LABELS].sum() / len(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361aab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final label distribution:\n",
      "love        8191\n",
      "anger       8084\n",
      "joy         7983\n",
      "sadness     6758\n",
      "surprise    5514\n",
      "fear        3197\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  freqs = label_frequencies(df)\n",
    "  max_label = freqs.idxmax()\n",
    "  max_frac = freqs[max_label]\n",
    "\n",
    "  if max_frac <= MAX_LABEL_FRACTION:\n",
    "    break\n",
    "\n",
    "  # find rows dominated only by the max label\n",
    "  mask = (df[max_label] == 1) & (df[KEPT_LABELS].sum(axis=1) == 1)\n",
    "  removable = df[mask]\n",
    "\n",
    "  if len(removable) == 0:\n",
    "    break\n",
    "\n",
    "  # remove a small chunk\n",
    "  drop_n = min(50, len(removable))\n",
    "  drop_idx = removable.sample(n=drop_n, random_state=RANDOM_SEED).index\n",
    "  df = df.drop(drop_idx)\n",
    "  \n",
    "\n",
    "print(\"Final label distribution:\")\n",
    "print(df[KEPT_LABELS].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abbe773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 3000\n"
     ]
    }
   ],
   "source": [
    "if len(df) > TARGET_MAX_SAMPLES:\n",
    "  df = df.sample(n=TARGET_MAX_SAMPLES, random_state=RANDOM_SEED)\n",
    "\n",
    "elif len(df) < TARGET_MIN_SAMPLES:\n",
    "  print(\"Warning: dataset smaller than target minimum\")\n",
    "\n",
    "print(f\"Final dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e55c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frozen evaluation set → data/Emotion NLP/goemotions_eval_set.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"data/Emotion NLP/goemotions_eval_set.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved frozen evaluation set → {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5568b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1de8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, texts, tokenizer, max_length=128):\n",
    "    self.texts = texts\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.tokenizer(\n",
    "      self.texts[idx],\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      max_length=self.max_length,\n",
    "      return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cba019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick inference\n",
    "def run_inference(model_name, model, tokenizer, texts, threshold=0.5):\n",
    "  print(f\"\\nRunning inference: {model_name}\")\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"Device: {device}\")\n",
    "\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits)  # multi-label probabilities\n",
    "    preds = (probs >= threshold).int()\n",
    "    \n",
    "  return preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6edfe35",
   "metadata": {},
   "source": [
    "possibly remove the batching?  \n",
    " |  \n",
    " v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b286705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_with_timing(model_name, model, dataloader, n_samples, threshold=0.5, batch_size=16):\n",
    "  print(f\"\\nRunning inference: {model_name}\")\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"Device: {device}\")\n",
    "\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  all_probs = []\n",
    "\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "      batch = {k: v.squeeze(1).to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      logits = outputs.logits\n",
    "      probs = torch.sigmoid(logits)\n",
    "      all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "  end_time = time.perf_counter()\n",
    "\n",
    "  probs = np.vstack(all_probs)\n",
    "\n",
    "  preds = (probs >= threshold).astype(int)\n",
    "\n",
    "  timing = {\n",
    "    \"model\": model_name,\n",
    "    \"device\": str(device),\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_samples\": n_samples,\n",
    "    \"total_inference_time_sec\": round(end_time - start_time, 3),\n",
    "    \"avg_time_per_sample_ms\": round(\n",
    "      (end_time - start_time) / n_samples * 1000, 3\n",
    "    )\n",
    "  }\n",
    "\n",
    "  return preds, probs, timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbc9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nlp_emotion_res(model_name, texts, labels, preds, probs, out_dir):\n",
    "  out_df = pd.DataFrame({\n",
    "    \"text\": texts\n",
    "  })\n",
    "\n",
    "  for i, label in enumerate(labels):\n",
    "    out_df[f\"prob_{label}\"] = probs[:, i]\n",
    "    out_df[f\"pred_{label}\"] = preds[:, i]\n",
    "\n",
    "  # Save predictions\n",
    "  pred_path = f\"{out_dir}/{model_name}_predictions.csv\"\n",
    "  out_df.to_csv(pred_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a8ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nlp_emotion_timing(model_name, timing, out_dir):\n",
    "  timing_path = f\"{out_dir}/{model_name}_timing.json\"\n",
    "  with open(timing_path, \"w\") as f:\n",
    "    json.dump(timing, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0198ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTIL_LABELS = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "ROBERTA_LABELS = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "MODEL_DATA = {\n",
    "  \"distilbert\": [\"bhadresh-savani/distilbert-base-uncased-emotion\", DISTIL_LABELS],\n",
    "  \"roberta\": [\"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\", ROBERTA_LABELS],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a4f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "THRESHOLD = 0.5\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7127aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = \"data/Emotion NLP/goemotions_eval_set.csv\"\n",
    "OUTPUT_DIR = \"data/Emotion NLP/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30dc254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 samples\n"
     ]
    }
   ],
   "source": [
    "goemotions_processed_df = pd.read_csv(PROCESSED_DATA_PATH)\n",
    "goemotions_texts = goemotions_processed_df[\"text\"].tolist()\n",
    "\n",
    "n_goemotions_samples = len(goemotions_processed_df)\n",
    "print(f\"Loaded {n_goemotions_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e58bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "distil_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load model\n",
    "distil_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "  \"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "  problem_type=\"multi_label_classification\",  # for multi-label\n",
    "  num_labels=len(DISTIL_LABELS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d400c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load model\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
    "  \"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\",\n",
    "  problem_type=\"multi_label_classification\",\n",
    "  num_labels=len(ROBERTA_LABELS)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfcedf",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a96e7",
   "metadata": {},
   "source": [
    "### Without Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc4b7c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
      "\n",
      "Running inference: distilbert\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0]], dtype=torch.int32),\n",
       " tensor([[0.2172, 0.9986, 0.2412, 0.1349, 0.0895, 0.1017],\n",
       "         [0.9974, 0.1571, 0.0907, 0.4971, 0.2588, 0.0890]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(DISTIL_LABELS)\n",
    "run_inference(\"distilbert\", distil_model, distil_tokenizer, [\"I am so happy today!\", \"This is terrible...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ea5bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
      "\n",
      "Running inference: roberta\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]], dtype=torch.int32),\n",
       " tensor([[0.0148, 0.0433, 0.0136, 0.0122, 0.9900, 0.6960, 0.8425, 0.0085, 0.0150,\n",
       "          0.0384, 0.1132],\n",
       "         [0.9562, 0.0266, 0.9608, 0.6311, 0.0089, 0.0081, 0.0064, 0.1003, 0.5182,\n",
       "          0.0625, 0.0075]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ROBERTA_LABELS)\n",
    "run_inference(\"roberta\", roberta_model, roberta_tokenizer, [\"I am so happy today!\", \"This is terrible...\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36328eb",
   "metadata": {},
   "source": [
    "### With Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268686dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SANITY_N = 5\n",
    "goemotions_sanity_df = goemotions_processed_df.head(SANITY_N)\n",
    "goemotions_sanity_texts = goemotions_sanity_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53504ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_goemotions_processed_distil_dataset = TextDataset(goemotions_sanity_texts, distil_tokenizer)\n",
    "sanity_distil_dataloader = DataLoader(sanity_goemotions_processed_distil_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "sanity_goemotions_processed_roberta_dataset = TextDataset(goemotions_sanity_texts, roberta_tokenizer)\n",
    "sanity_roberta_dataloader = DataLoader(sanity_goemotions_processed_roberta_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f6718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: distilbert\n",
      "Device: cpu\n",
      "Probabilities shape: (5, 6)\n",
      "Sample probabilities:\n",
      " [[0.9964663  0.1493925  0.05684914 0.25949445 0.6377073  0.08722472]\n",
      " [0.57768834 0.7492968  0.03682164 0.9190556  0.71199644 0.03879093]]\n",
      "Sample predictions:\n",
      " [[1 0 0 0 1 0]\n",
      " [1 1 0 1 1 0]]\n",
      "\n",
      "TEXT: This, in top of my already terrible mental health, is the reason why I'm severely underweight and need help.\n",
      "sadness    → 0.996\n",
      "joy        → 0.149\n",
      "love       → 0.057\n",
      "anger      → 0.259\n",
      "fear       → 0.638\n",
      "surprise   → 0.087\n",
      "\n",
      "TEXT: We don't know anything yet till it happens. It's great to see everything think they know the eliminations order.....lol. you will be shooked\n",
      "sadness    → 0.578\n",
      "joy        → 0.749\n",
      "love       → 0.037\n",
      "anger      → 0.919\n",
      "fear       → 0.712\n",
      "surprise   → 0.039\n",
      "\n",
      "TEXT: How poignant.\n",
      "sadness    → 0.256\n",
      "joy        → 0.894\n",
      "love       → 0.556\n",
      "anger      → 0.850\n",
      "fear       → 0.057\n",
      "surprise   → 0.152\n",
      "\n",
      "TEXT: Loved dropping in from the crack in the ceiling and going out in a blaze of glory.\n",
      "sadness    → 0.152\n",
      "joy        → 0.839\n",
      "love       → 0.994\n",
      "anger      → 0.141\n",
      "fear       → 0.051\n",
      "surprise   → 0.142\n",
      "\n",
      "TEXT: I've said it before, but yuck, kissing that bloody mouth... Watch out, [NAME], you don't know where that's been.\n",
      "sadness    → 0.558\n",
      "joy        → 0.197\n",
      "love       → 0.078\n",
      "anger      → 0.994\n",
      "fear       → 0.390\n",
      "surprise   → 0.067\n"
     ]
    }
   ],
   "source": [
    "preds, probs, timing = run_inference_with_timing(\n",
    "  \"distilbert-santity\",\n",
    "  distil_model,\n",
    "  sanity_distil_dataloader,\n",
    "  n_goemotions_samples\n",
    ")\n",
    "\n",
    "print(\"Probabilities shape:\", probs.shape)\n",
    "print(\"Sample probabilities:\\n\", probs[:2])\n",
    "\n",
    "print(\"Sample predictions:\\n\", preds[:2])\n",
    "\n",
    "for i in range(goemotions_sanity_df.shape[0]):\n",
    "  print(\"\\nTEXT:\", goemotions_sanity_texts[i])\n",
    "  for j, label in enumerate(DISTIL_LABELS):\n",
    "    print(f\"{label:10s} → {probs[i][j]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59feea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: roberta\n",
      "Device: cpu\n",
      "Probabilities shape: (5, 11)\n",
      "Sample probabilities:\n",
      " [[0.07512293 0.03976237 0.17263333 0.72579306 0.01285184 0.01303598\n",
      "  0.03384648 0.6592022  0.9587615  0.01310243 0.01232171]\n",
      " [0.02888189 0.3025471  0.03123977 0.04462678 0.9852583  0.12936689\n",
      "  0.54414463 0.00778534 0.01056351 0.29578766 0.08114873]]\n",
      "Sample predictions:\n",
      " [[0 0 0 1 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 0]]\n",
      "\n",
      "TEXT: This, in top of my already terrible mental health, is the reason why I'm severely underweight and need help.\n",
      "sadness    → 0.075\n",
      "joy        → 0.040\n",
      "love       → 0.173\n",
      "anger      → 0.726\n",
      "fear       → 0.013\n",
      "surprise   → 0.013\n",
      "\n",
      "TEXT: We don't know anything yet till it happens. It's great to see everything think they know the eliminations order.....lol. you will be shooked\n",
      "sadness    → 0.029\n",
      "joy        → 0.303\n",
      "love       → 0.031\n",
      "anger      → 0.045\n",
      "fear       → 0.985\n",
      "surprise   → 0.129\n",
      "\n",
      "TEXT: How poignant.\n",
      "sadness    → 0.009\n",
      "joy        → 0.044\n",
      "love       → 0.021\n",
      "anger      → 0.011\n",
      "fear       → 0.974\n",
      "surprise   → 0.291\n",
      "\n",
      "TEXT: Loved dropping in from the crack in the ceiling and going out in a blaze of glory.\n",
      "sadness    → 0.024\n",
      "joy        → 0.031\n",
      "love       → 0.025\n",
      "anger      → 0.021\n",
      "fear       → 0.978\n",
      "surprise   → 0.936\n",
      "\n",
      "TEXT: I've said it before, but yuck, kissing that bloody mouth... Watch out, [NAME], you don't know where that's been.\n",
      "sadness    → 0.979\n",
      "joy        → 0.044\n",
      "love       → 0.938\n",
      "anger      → 0.081\n",
      "fear       → 0.021\n",
      "surprise   → 0.006\n"
     ]
    }
   ],
   "source": [
    "preds, probs, timing = run_inference_with_timing(\n",
    "  \"roberta-sanity\",\n",
    "  roberta_model,\n",
    "  sanity_roberta_dataloader,\n",
    "  n_goemotions_samples\n",
    ")\n",
    "\n",
    "print(\"Probabilities shape:\", probs.shape)\n",
    "print(\"Sample probabilities:\\n\", probs[:2])\n",
    "\n",
    "print(\"Sample predictions:\\n\", preds[:2])\n",
    "\n",
    "for i in range(goemotions_sanity_df.shape[0]):\n",
    "  print(\"\\nTEXT:\", goemotions_sanity_texts[i])\n",
    "  for j, label in enumerate(DISTIL_LABELS):\n",
    "    print(f\"{label:10s} → {probs[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d54b52",
   "metadata": {},
   "source": [
    "## Full Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6687e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "goemotions_processed_distil_dataset = TextDataset(goemotions_texts, distil_tokenizer)\n",
    "distil_dataloader = DataLoader(goemotions_processed_distil_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "goemotions_processed_roberta_dataset = TextDataset(goemotions_texts, roberta_tokenizer)\n",
    "roberta_dataloader = DataLoader(goemotions_processed_roberta_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef131dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: distilbert-goemotions\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-goemotions\"\n",
    "distil_preds, distil_probs, distil_timing = run_inference_with_timing(\n",
    "  model_name,\n",
    "  distil_model,\n",
    "  distil_dataloader,\n",
    "  n_goemotions_samples\n",
    ")\n",
    "save_nlp_emotion_res(model_name, \n",
    "                     goemotions_texts, \n",
    "                     DISTIL_LABELS, \n",
    "                     distil_preds, \n",
    "                     distil_probs,\n",
    "                     OUTPUT_DIR)\n",
    "save_nlp_emotion_timing(model_name, distil_timing, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-goemotions\"\n",
    "roberta_preds, roberta_probs, roberta_timing = run_inference_with_timing(\n",
    "  model_name,\n",
    "  roberta_model,\n",
    "  roberta_dataloader,\n",
    "  n_goemotions_samples\n",
    ")\n",
    "save_nlp_emotion_res(model_name,\n",
    "                     goemotions_texts, \n",
    "                     ROBERTA_LABELS, \n",
    "                     roberta_preds, \n",
    "                     roberta_probs, \n",
    "                     roberta_timing,\n",
    "                     OUTPUT_DIR)\n",
    "save_nlp_emotion_timing(model_name, roberta_timing, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482deae1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e25478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel(\n",
    "  df_preds,\n",
    "  df_gold,\n",
    "  labels,\n",
    "  threshold=0.5):\n",
    "  \"\"\"\n",
    "  df_preds: dataframe with conf_<label> columns\n",
    "  df_gold: dataframe with true <label> columns\n",
    "  labels: list of labels to evaluate\n",
    "  \"\"\"\n",
    "\n",
    "  # Build prediction matrix\n",
    "  y_pred = np.vstack([\n",
    "    (df_preds[f\"conf_{lbl}\"] >= threshold).astype(int)\n",
    "    for lbl in labels\n",
    "  ]).T\n",
    "\n",
    "  # Build gold matrix\n",
    "  y_true = df_gold[labels].values\n",
    "\n",
    "  # Macro + micro\n",
    "  macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"macro\", zero_division=0\n",
    "  )\n",
    "\n",
    "  micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"micro\", zero_division=0\n",
    "  )\n",
    "\n",
    "  overall = pd.DataFrame([{\n",
    "    \"macro_precision\": macro_p,\n",
    "    \"macro_recall\": macro_r,\n",
    "    \"macro_f1\": macro_f1,\n",
    "    \"micro_precision\": micro_p,\n",
    "    \"micro_recall\": micro_r,\n",
    "    \"micro_f1\": micro_f1\n",
    "  }])\n",
    "\n",
    "  # Per-label metrics\n",
    "  per_label = []\n",
    "  for i, lbl in enumerate(labels):\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "      y_true[:, i],\n",
    "      y_pred[:, i],\n",
    "      average=\"binary\",\n",
    "      zero_division=0\n",
    "    )\n",
    "    per_label.append({\n",
    "      \"label\": lbl,\n",
    "      \"precision\": p,\n",
    "      \"recall\": r,\n",
    "      \"f1\": f1\n",
    "    })\n",
    "\n",
    "  per_label_df = pd.DataFrame(per_label)\n",
    "\n",
    "  return overall, per_label_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT - Overall\n",
      "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
      "0         0.413491      0.605861  0.435764         0.357182       0.62784   \n",
      "\n",
      "   micro_f1  \n",
      "0  0.455326  \n",
      "\n",
      "DistilBERT - Per emotion F1\n",
      "      label  precision    recall        f1\n",
      "4  surprise   0.512438  0.227876  0.315467\n",
      "3      fear   0.251603  0.554770  0.346196\n",
      "0       joy   0.310972  0.879421  0.459471\n",
      "2     anger   0.327097  0.889984  0.478376\n",
      "5      love   0.666667  0.401899  0.501481\n",
      "1   sadness   0.412170  0.681214  0.513591\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413491</td>\n",
       "      <td>0.605861</td>\n",
       "      <td>0.435764</td>\n",
       "      <td>0.357182</td>\n",
       "      <td>0.62784</td>\n",
       "      <td>0.455326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
       "0         0.413491      0.605861  0.435764         0.357182       0.62784   \n",
       "\n",
       "   micro_f1  \n",
       "0  0.455326  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distil_preds = pd.read_csv(\"data/Emotion NLP/results/distilbert_predictions.csv\")\n",
    "df_gold = pd.read_csv(\"data/Emotion NLP/goemotions_eval_set.csv\")\n",
    "\n",
    "distil_overall, distil_per_label = evaluate_multilabel(\n",
    "  df_preds=df_distil_preds,\n",
    "  df_gold=df_gold,\n",
    "  labels=KEPT_LABELS,\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"DistilBERT - Overall\")\n",
    "print(distil_overall)\n",
    "\n",
    "print(\"\\nDistilBERT - Per emotion F1\")\n",
    "print(distil_per_label.sort_values(\"f1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "651fe1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_roberta_predictions(\n",
    "  df_roberta,\n",
    "  mapping,\n",
    "  threshold=0.5):\n",
    "  \"\"\"\n",
    "  Converts RoBERTa predictions into 6-label emotion space\n",
    "  \"\"\"\n",
    "\n",
    "  df_out = df_roberta[[\"id\", \"text\"]].copy()\n",
    "\n",
    "  for target_label, source_labels in mapping.items():\n",
    "    conf_cols = [f\"conf_{lbl}\" for lbl in source_labels]\n",
    "\n",
    "    # Max confidence across mapped labels\n",
    "    df_out[f\"conf_{target_label}\"] = df_roberta[conf_cols].max(axis=1)\n",
    "\n",
    "    # Binary prediction\n",
    "    df_out[f\"pred_{target_label}\"] = (\n",
    "      df_out[f\"conf_{target_label}\"] >= threshold\n",
    "    ).astype(int)\n",
    "\n",
    "  return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ddcefbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa - Overall\n",
      "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
      "0         0.518823       0.70346  0.577528         0.492351        0.7312   \n",
      "\n",
      "   micro_f1  \n",
      "0  0.588463  \n",
      "\n",
      "RoBERTa - Per emotion F1\n",
      "      label  precision    recall        f1\n",
      "3      fear   0.368664  0.565371  0.446304\n",
      "4  surprise   0.619632  0.446903  0.519280\n",
      "0       joy   0.388453  0.876206  0.538272\n",
      "2     anger   0.465442  0.873563  0.607306\n",
      "1   sadness   0.558651  0.722960  0.630273\n",
      "5      love   0.712098  0.735759  0.723735\n"
     ]
    }
   ],
   "source": [
    "ROBERTA_TO_6 = {\n",
    "  \"anger\": [\"anger\", \"disgust\"],\n",
    "  \"fear\": [\"fear\", \"pessimism\"],\n",
    "  \"joy\": [\"joy\", \"optimism\"],\n",
    "  \"love\": [\"love\", \"trust\"],\n",
    "  \"sadness\": [\"sadness\"],\n",
    "  \"surprise\": [\"surprise\", \"anticipation\"]\n",
    "}\n",
    "\n",
    "\n",
    "df_roberta_preds = pd.read_csv(\"data/Emotion NLP/results/roberta_predictions.csv\")\n",
    "\n",
    "df_roberta_6 = collapse_roberta_predictions(\n",
    "  df_roberta_preds,\n",
    "  ROBERTA_TO_6,\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "df_roberta_6.head()\n",
    "\n",
    "roberta_overall, roberta_per_label = evaluate_multilabel(\n",
    "  df_preds=df_roberta_6,\n",
    "  df_gold=df_gold,\n",
    "  labels=KEPT_LABELS,\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"RoBERTa - Overall\")\n",
    "print(roberta_overall)\n",
    "\n",
    "print(\"\\nRoBERTa - Per emotion F1\")\n",
    "print(roberta_per_label.sort_values(\"f1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbe30a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413491</td>\n",
       "      <td>0.605861</td>\n",
       "      <td>0.435764</td>\n",
       "      <td>0.357182</td>\n",
       "      <td>0.62784</td>\n",
       "      <td>0.455326</td>\n",
       "      <td>DistilBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.590085</td>\n",
       "      <td>0.659299</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.522958</td>\n",
       "      <td>0.69248</td>\n",
       "      <td>0.595897</td>\n",
       "      <td>RoBERTa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   macro_precision  macro_recall  macro_f1  micro_precision  micro_recall  \\\n",
       "0         0.413491      0.605861  0.435764         0.357182       0.62784   \n",
       "0         0.590085      0.659299  0.574200         0.522958       0.69248   \n",
       "\n",
       "   micro_f1       model  \n",
       "0  0.455326  DistilBERT  \n",
       "0  0.595897     RoBERTa  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_common_overall, distil_common_per_label = evaluate_multilabel(\n",
    "  df_distil_preds, df_gold, KEPT_LABELS\n",
    ")\n",
    "\n",
    "roberta_common_overall, roberta_common_per_label = evaluate_multilabel(\n",
    "  df_roberta_preds, df_gold, KEPT_LABELS\n",
    ")\n",
    "\n",
    "comparison = pd.concat([\n",
    "  distil_common_overall.assign(model=\"DistilBERT\"),\n",
    "  roberta_common_overall.assign(model=\"RoBERTa\")\n",
    "])\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be2084",
   "metadata": {},
   "source": [
    "# Real World Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8a45a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = pd.read_csv(\"data/Emotion NLP/journal_texts/journal_texts.csv\")\n",
    "journal_texts_list = journal_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4448625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: distilbert-journal\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-journal\"\n",
    "distil_preds, distil_probs = run_inference(\n",
    "    model_name,\n",
    "    distil_model,\n",
    "    distil_tokenizer,\n",
    "    journal_texts_list)\n",
    "save_nlp_emotion_res(model_name, \n",
    "                     journal_texts_list, \n",
    "                     DISTIL_LABELS, \n",
    "                     distil_preds, \n",
    "                     distil_probs,\n",
    "                     OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e77b9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference: roberta-journal\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-journal\"\n",
    "roberta_preds, roberta_probs = run_inference(\n",
    "    model_name, \n",
    "    roberta_model, \n",
    "    roberta_tokenizer, \n",
    "    journal_texts_list)\n",
    "save_nlp_emotion_res(model_name, \n",
    "                     journal_texts_list, \n",
    "                     ROBERTA_LABELS, \n",
    "                     roberta_preds, \n",
    "                     roberta_probs,\n",
    "                     OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436edcd3",
   "metadata": {},
   "source": [
    "## Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4cbf7",
   "metadata": {},
   "source": [
    "Bucket A - High confidence, plausible  \n",
    "Bucket B - High confidence, wrong  \n",
    "Bucket C - Low confidence or ambigious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa09673",
   "metadata": {},
   "source": [
    "DistilBERT\n",
    "| Bucket | Text | Model behaviour | Rationale |\n",
    "| - | - | - | - |\n",
    "| A | I felt tired even after sleeping. Emotionally drained is the only way I can describe it | Sadness (0.999) | Clear match between text and prediction |\n",
    "| A | There was a moment today where I felt calm. It didn’t last long, but it reminded me that calm is still possible | Joy (0.998) | Calm plausibly mapped to positive affect |\n",
    "| A | It felt uncomfortable to sit with my emotions instead of distracting myself, but I tried anyway | Fear (0.996) | Emotional discomfort reasonably interpreted |\n",
    "| B | Today felt heavier than I expected. I kept replaying the conversation in my head, wondering if I said too much or not enough | Fear, sadness, anger, joy all high | Overconfident multi-label prediction |\n",
    "| B | Some feelings are hard to name. I know something is there, but I can’t quite explain it. | Fear, anger, joy all high | Explicit ambiguity ignored |\n",
    "| B | I noticed a pattern in my reactions today. I get defensive when I feel misunderstood. | Sadness, fear, anger high | Introspection treated as crisis |\n",
    "| C | I noticed how tense my body felt this morning. My shoulders were tight, and I struggled to slow my breathing. | Fear dominant | Somatic stress ambiguous |\n",
    "| C | There was a sense of restlessness throughout the day, like I couldn’t fully settle into anything. | Fear dominant | Restlessness confused with fear |\n",
    "| C | I'm not sure what I was feeling exactly, just that it stayed with me longer than I expected. | Joy dominant | Confidence contradicts uncertainty |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af663160",
   "metadata": {},
   "source": [
    "RoBERTa\n",
    "| Bucket | Text | Model behaviour | Rationale |\n",
    "| - | - | - | - |\n",
    "| A | I felt frustrated during the session, but I didn’t say it out loud. I’m not sure why I held back. | Anger (0.921) | Explicit emotion correctly detected |\n",
    "| A | I felt tired even after sleeping. Emotionally drained is the only way I can describe it | Sadness (0.979) | Clear emotional exhaustion |\n",
    "| A | There was a quiet sense of relief when I finally had time alone this evening. | Joy, optimism high | Appropriate positive affect |\n",
    "| B | Today felt heavier than I expected. I kept replaying the conversation in my head, wondering if I said too much or not enough | Fear (0.893) | Rumination ≠ fear |\n",
    "| B | Today wasn’t terrible, but it also wasn’t good. It just felt flat. | Sadness, disgust high | Neutral affect over-interpreted |\n",
    "| B | Some feelings are hard to name. I know something is there, but I can’t quite explain it. | Sadness high | Uncertainty ignored |\n",
    "| C | I caught myself overthinking small interactions today, even though nothing objectively bad happened. | Sadness, optimism mixed | No dominant signal |\n",
    "| C | Part of me wanted reassurance, but another part didn’t want to ask for it. | Sadness moderate | Vulnerability ambiguous |\n",
    "| C | I'm not sure what I was feeling exactly, just that it stayed with me longer than I expected. | Anticipation weak | Low-confidence diffuse emotion |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
